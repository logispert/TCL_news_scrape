{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is written by Hoplin\n",
    "# Code last edited : 2020/06/20\n",
    "# Description : This code will be used as algorithm of 'The Camp' News Sending program  \n",
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import URLError\n",
    "from urllib.request import HTTPError\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "# Save News Datas : Data include articles, title\n",
    "newsDatas = dict()\n",
    "# News Topics will be save here\n",
    "newsTopics = []\n",
    "\n",
    "def returnHyperTextAddingHeader(URL):\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "    }\n",
    "    html = session.get(URL,headers=headers).content \n",
    "    html = BeautifulSoup(html,\"html.parser\")\n",
    "    return html\n",
    "\n",
    "#각 섹션별 기사URL에 대해 기사 출력\n",
    "def getArticleMainText(URL):\n",
    "    '''\n",
    "    html = urlopen(URL)\n",
    "    html = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    왜 이부분을 주석처리하고 사용하지 않나요?\n",
    "    이 전에 다음과 같은 오류가 발생했었습니다.\n",
    "    http.client.RemoteDisconnected: Remote end closed connection without response\n",
    "    이러한 오류가 발생하는 이유는 요청을 받는 서버에서 요청을 비정상적이 요청으로 인지하고\n",
    "    차단하는 경우에 발생하게 된다. 물론 서버 마다 다르지만 이렇게 차단된 상태의 html을 출력해 보면\n",
    "    '비정상적인 요청에 따른 일시적 제한' 등의 글이 써져있는것을 볼 수 있다.\n",
    "    결론적으로 말하면 뷰봇과 같은 '봇'으로 인식을 한 것이다.\n",
    "    이렇게 봇으로 인식하는것을 방지하기 위해서는 패킷 header 정보에 User-Agent 정보를 넣어주면된다.\n",
    "    '''\n",
    "\n",
    "    html = returnHyperTextAddingHeader(URL)\n",
    "    # 본문 내용은 id가 articleBodyContents 인 <div> 태그 안에있다.\n",
    "    paragraphElements = re.sub(' +', ' ', html.find('div',{'id' : 'articleBodyContents'}).text).strip() # ' +' : white space가 한개 이상인 패턴을 찾는다.\n",
    "    return paragraphElements\n",
    "    \n",
    "def JSONConverter(dataCapsule):\n",
    "    with open('NaverNewsHeadlineScrape_' + str(time.strftime('%Y-%m-%d', time.localtime(time.time()))) + '.json','w',encoding=\"utf-8\") as make_file:\n",
    "        json.dump(dataCapsule,make_file,ensure_ascii=False,indent=\"\\t\") # indent : 들여쓰기값 Indent Parameter\n",
    "\n",
    "try:\n",
    "    naverNewsURL = 'https://news.naver.com/'\n",
    "    #\"https://news.naver.com/main/list.nhn?mode=LS2D&sid2={0}&sid1=101&mid=shm&date={1}&{2}\".format(\"259\", \"20200408\", \"1\")\n",
    "    html = returnHyperTextAddingHeader(naverNewsURL)\n",
    "    '''\n",
    "    Get elements : About each news section. There are 7 sections. \n",
    "    \n",
    "    1. Today headline news \n",
    "    \n",
    "    2. Economy\n",
    "    \n",
    "    3. Society\n",
    "    \n",
    "    4. Life\n",
    "    \n",
    "    5. World\n",
    "    \n",
    "    6. IT\n",
    "    \n",
    "    7. Politics\n",
    "    '''\n",
    "    sectionElements = html.find('div',{'class' : 'main_content_inner _content_inner'}).findAll('div',{'class' : re.compile('main\\_component[A-Za-z]*')})\n",
    "    #print(sectionElements)\n",
    "    del sectionElements[-1] # 맨 마지막에 필요 없는 데이터를 제거한다.\n",
    "    \n",
    "    # Last Scraped Time.\n",
    "    newsDatas['lastScrapedTime'] = time.strftime('%c', time.localtime(time.time())) \n",
    "    # Source\n",
    "    newsDatas['DataSource'] = 'Naver News'\n",
    "    \n",
    "    for topic in sectionElements:\n",
    "        capsule = dict()\n",
    "        topicName = topic.find('h4').text\n",
    "        \n",
    "        articlesInformation = topic.findAll('li')\n",
    "        \n",
    "        for inf in range(len(articlesInformation)):\n",
    "            articleURL = articlesInformation[inf].find('a')['href']\n",
    "            articleURLPatternChecker = re.compile('\\/main\\/[A-Za-z0-9]*')\n",
    "            checkerBoolean = articleURLPatternChecker.match(articleURL)\n",
    "            \n",
    "            # Article Text Scrape Here\n",
    "            \n",
    "            # Headline News's URL Type : Full URL \n",
    "            # Other sections' News's URL Type : /main/~~~ \n",
    "            articText = None\n",
    "            if checkerBoolean:\n",
    "                articText = getArticleMainText('https://news.naver.com' + articleURL)\n",
    "            else:\n",
    "                articText = getArticleMainText(articleURL)\n",
    "            capsule['article' + '_' +str(inf + 1)] = {\n",
    "                'articleTitle' : articlesInformation[inf].find('a').text.strip().replace(\"\\\"\",\"'\"),\n",
    "                'url' : articleURL,\n",
    "                'articleText' : articText.replace(\"\\t\",\"\").replace(\"\\n\",\"\").replace(\"\\\"\",\"'\")\n",
    "            }\n",
    "                \n",
    "        newsDatas[topicName] = capsule\n",
    "    \n",
    "    JSONConverter(newsDatas)\n",
    "except URLError as errormessageURL:\n",
    "    print(errormessageURL)\n",
    "except HTTPError as errormessageHTTP:\n",
    "    print(errormessageHTTP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=421&aid=0004573555\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=468&aid=0000645976\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=366&aid=0000506782\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=119&aid=0002394759\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=018&aid=0004616099\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=277&aid=0004658238\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=014&aid=0004405226\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=014&aid=0004405223\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=421&aid=0004573521\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=119&aid=0002394758\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=011&aid=0003721608\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=015&aid=0004321092\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=029&aid=0002592826\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=018&aid=0004616059\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=018&aid=0004616058\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=029&aid=0002592825\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=028&aid=0002492794\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=014&aid=0004405233\n",
      "https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=101&sid2=259&oid=468&aid=0000645974\n"
     ]
    }
   ],
   "source": [
    "# This code is written by Hoplin\n",
    "# Code last edited : 2020/06/20\n",
    "# Description : This code will be used as algorithm of 'The Camp' News Sending program  \n",
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import URLError\n",
    "from urllib.request import HTTPError\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import ssl\n",
    "\n",
    "# Save News Datas : Data include articles, title\n",
    "newsDatas = dict()\n",
    "# News Topics will be save here\n",
    "newsTopics = []\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "}\n",
    "\n",
    "#headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "\n",
    "def returnHyperTextAddingHeader(URL):\n",
    "    session = requests.Session()\n",
    "\n",
    "    html = session.get(URL,headers=headers).content \n",
    "    html = BeautifulSoup(html,\"html.parser\")\n",
    "    return html\n",
    "\n",
    "#각 섹션별 기사URL에 대해 기사 출력\n",
    "def getArticleMainText(URL):\n",
    "    '''\n",
    "    html = urlopen(URL)\n",
    "    html = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    왜 이부분을 주석처리하고 사용하지 않나요?\n",
    "    이 전에 다음과 같은 오류가 발생했었습니다.\n",
    "    http.client.RemoteDisconnected: Remote end closed connection without response\n",
    "    이러한 오류가 발생하는 이유는 요청을 받는 서버에서 요청을 비정상적이 요청으로 인지하고\n",
    "    차단하는 경우에 발생하게 된다. 물론 서버 마다 다르지만 이렇게 차단된 상태의 html을 출력해 보면\n",
    "    '비정상적인 요청에 따른 일시적 제한' 등의 글이 써져있는것을 볼 수 있다.\n",
    "    결론적으로 말하면 뷰봇과 같은 '봇'으로 인식을 한 것이다.\n",
    "    이렇게 봇으로 인식하는것을 방지하기 위해서는 패킷 header 정보에 User-Agent 정보를 넣어주면된다.\n",
    "    '''\n",
    "\n",
    "    html = returnHyperTextAddingHeader(URL)\n",
    "    # 본문 내용은 id가 articleBodyContents 인 <div> 태그 안에있다.\n",
    "    paragraphElements = re.sub(' +', ' ', html.find('div',{'id' : 'articleBodyContents'}).text).strip() # ' +' : white space가 한개 이상인 패턴을 찾는다.\n",
    "    return paragraphElements    \n",
    "\n",
    "def JSONConverter(dataCapsule):\n",
    "    with open('NaverNewsDailyScrape_' + str(time.strftime('%Y-%m-%d', time.localtime(time.time()))) + '.json','w',encoding=\"utf-8\") as make_file:\n",
    "        json.dump(dataCapsule,make_file,ensure_ascii=False,indent=\"\\t\") # indent : 들여쓰기값 Indent Parameter\n",
    "\n",
    "try:\n",
    "    idx = 0\n",
    "    # 네이버 뉴스 - 경제(101) - 금융(259)\n",
    "    target_url = \"https://news.naver.com/main/list.nhn?mode=LS2D&sid2={0}&sid1=101&mid=shm&date={1}&{2}\".format(\"259\", \"20200408\", \"1\") \n",
    "    context = ssl._create_unverified_context()\n",
    "    source_code = requests.get(target_url, headers = headers)\n",
    "\n",
    "    soup = BeautifulSoup(source_code.text, \"html.parser\")\n",
    "    url_set = set()\n",
    "    for item in soup.find(id='main_content').find_all(\"a\"):\n",
    "        if item.has_attr(\"href\"):\n",
    "            if str(item.get(\"href\")).startswith(\"http\"):\n",
    "    #             print(str(item.get(\"href\")).find(\"sid1\"))\n",
    "    #             print(str(item.get(\"href\"))[str(item.get(\"href\")).find(\"sid1\"):])\n",
    "                url_set.add(str(item.get(\"href\")))\n",
    " \n",
    "    for url in url_set:\n",
    "        \n",
    "        #html은 내리지 않고 바로 json만 내리고 싶다면 아래 82-89블럭은 주석처리\n",
    "        #html파일내리기 start\n",
    "        print(url)\n",
    "        source_code = requests.get(url, headers = headers)\n",
    "        html_text = str(BeautifulSoup(source_code.text, \"html.parser\"))\n",
    "        html_file = open(\"data/\" + url[url.find(\"sid1\"):] + '.html', 'w',-1,'utf-8')\n",
    "        html_file.write(html_text)\n",
    "        html_file.close()\n",
    "        #end of html파일 내리기\n",
    "\n",
    "        #json파일 만들기\n",
    "        naverNewsURL = url\n",
    "        html = returnHyperTextAddingHeader(naverNewsURL)\n",
    "\n",
    "        idx+=1\n",
    "        \n",
    "        # Last Scraped Time.\n",
    "        newsDatas['lastScrapedTime'] = time.strftime('%c', time.localtime(time.time())) \n",
    "        # Source\n",
    "        newsDatas['DataSource'] = 'Naver News'\n",
    "\n",
    "        capsule = dict()\n",
    "        \n",
    "        topicName = '네이버뉴스-경제-금융-20200408-'\n",
    "        articleURL = url\n",
    "        articleTitle = html.find('h3').text\n",
    "        articText = getArticleMainText(url)\n",
    "\n",
    "        capsule['article' + '_' +str(idx)] = {\n",
    "            'articleTitle' : articleTitle.replace(\"\\\"\",\"'\"),\n",
    "            'url' : articleURL,\n",
    "            'articleText' : articText.replace(\"\\t\",\"\").replace(\"\\n\",\"\").replace(\"\\\"\",\"'\")\n",
    "        }\n",
    "        \n",
    "        newsDatas[topicName+str(idx)] = capsule\n",
    "    \n",
    "    JSONConverter(newsDatas)\n",
    " \n",
    "except URLError as errormessageURL:\n",
    "    print(errormessageURL)\n",
    "except HTTPError as errormessageHTTP:\n",
    "    print(errormessageHTTP)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
